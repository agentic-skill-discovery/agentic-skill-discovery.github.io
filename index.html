<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Agentic Skill Discovery
    </title>

    <meta name="description"
        content="Chat with the Environment: Interactive Multimodal Perception using Large Language Models">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/background.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/locomotive-scroll@4.1.3/dist/locomotive-scroll.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="js/app.js"></script>
    <!-- <script src="https://cdn.jsdelivr.net/npm/ukiyojs@4.0.3/dist/ukiyo.umd.min.js"> </script> -->

    <style>
        .nav-pills {
            position: relative;
            display: inline;
        }

        .imtip {
            position: absolute;
            top: 0;
            left: 0;
        }
    </style>
</head>

<body>
    <div id="parallax-world-of-ugg">

        <section>
            <div class="title">
                <h3>preprint</h3>
                <h1>Agentic Skill Discovery</h1>
            </div>
        </section>
          <video autoplay muted loop>
            <source src="img/ASD.mp4" type="video/mp4">
          </video>

        </br>
        <div class="container" id="main">
            <div class="row">
                <p class="col-md-12 text-center">
                    <font size="+3">â†“â†“</br> </font>
                    </br>
                </p>
                <div class="col-md-12 text-center">
                    (preprint)
                </div>
            </div>
            <div class="row">
                <div class="col-md-12 text-center">
                    <ul class="list-inline">
                        <br>
                        <li><a href="https://xf-zhao.github.io">Xufeng Zhao</a> </li>
                        <li><a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/weber.html">Cornelius
                                Weber</a></li>
                        <li><a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/wermter.html">Stefan
                                Wermter</a></li>
                        <br><br>
                        <a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm.html">
                            <image src="img/new-wtm-logo-white-150x150.jpg" height="40px"> Knowledge Technology Group, University of Hamburg
                        </a>
                    </ul>
                </div>
            </div>

            <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2405.15019">
                                <image src="img/paper_small.png" height="60px">
                                    <h4><strong>Paper</strong></h4>
                            </a>
                        </li>

                        <li>
                            <a href="https://github.com/xf-zhao/Agentic-Skill-Discovery">
                                <image src="img/github.png" height="60px">
                                    <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
            </div>

            <div class="row">
                <div class="col-md-8 text-left col-md-offset-2">
                    <h4>
                        ðŸ”” <b>News</b>
                    </h4>
                    <li> <a style="color:gray;">[2023-09-29]</a> Build this webpage, to add more videos and information later.</li>
                </div>
            </div>
            </br>
            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h4><b>
                        ðŸ“œ Abstract </b>
                    </h4>
                    <p class="text-justify">

                        Language-conditioned robotic skills make it possible to apply the high-level reasoning of Large Language Models (LLMs) to low-level robotic control. A remaining challenge is to acquire a diverse set of fundamental skills. Existing approaches either manually decompose a complex task into atomic robotic actions in a top-down fashion, or bootstrap as many combinations as possible in a bottom-up fashion to cover a wider range of task possibilities. These decompositions or combinations, however, require an initial skill library. For example, a "grasping" capability can never emerge from a skill library containing only diverse "pushing" skills. Existing skill discovery techniques with reinforcement learning acquire skills by an exhaustive exploration but often yield non-meaningful behaviors. In this study, we introduce a novel framework for skill discovery that is entirely driven by LLMs. The framework begins with an LLM generating task proposals based on the provided scene description and the robot's configurations, aiming to incrementally acquire new skills upon task completion. For each proposed task, a series of reinforcement learning processes are initiated, utilizing reward and success determination functions sampled by the LLM to develop the corresponding policy. The reliability and trustworthiness of learned behaviors are further ensured by an independent vision-language model. We show that starting with zero skill, the ASD skill library emerges and expands to more and more meaningful and reliable skills, enabling the robot to efficiently further propose and complete advanced tasks. 
                    </p>
                    </div>
                    </div>

            <div class="row">
                <div class="col-md-8 text-left col-md-offset-2">
                    <h4>
                        ðŸŒ„ <b>Video demo</b>
                    </h4>
                    <li>
                        To add.
                    </li>
                    </br>
                    </br>
                </div>
            </div>

            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h4><b>
                        Citation</b>
                    </h4> <a href="https://arxiv.org/abs/2405.15019">[arxiv version]</a>
                    <div class="form-group col-md-10 col-md-offset-1">
                        <textarea id="bibtex" class="form-control" readonly>
@article{zhao2024agentic,
    title={Agentic Skill Discovery},
    author={Zhao, Xufeng and Weber, Cornelius and Wermter, Stefan},
    journal={arXiv preprint arXiv:2405.15019},
    year={2024}
  }
                        </textarea>
                    </div>
                </div>
            </div>

            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    This website template borrowed from <a href="https://say-can.github.io/">say-can</a>,
                    but adapted with background animation. Feel free to fork <a
                        href="https://github.com/agentic-skill-discovery/agentic-skill-discovery.github.io">this template</a>.
                </div>
            </div>
        </div>
        </br>
</body>

</html>